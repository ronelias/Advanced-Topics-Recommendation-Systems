{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07f204d-b129-41bb-ad9d-e52b522c8148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbimporter\n",
      "  Downloading nbimporter-0.3.4-py3-none-any.whl.metadata (252 bytes)\n",
      "Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: nbimporter\n",
      "Successfully installed nbimporter-0.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nbimporter\n",
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86f3f90-9c78-42fa-8bd8-e83c1a7a01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f654d898-ee78-4a04-888e-4560dca4e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import architectures\n",
    "from recommenders_architecture import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b25565-a06b-4827-a30d-b4fb1aea74c4",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10fe0bfc-9192-4912-a6d7-f679269adca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Load Pairwise Training Data =======\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "pairwise_data_path= current_dir.parent / \"data\" / \"pairwise\"/\"pairwise_data.csv\"\n",
    "df = pd.read_csv(pairwise_data_path)\n",
    "\n",
    "# ======= Load Item Metadata (1027-dim vectors) =======\n",
    "models_dir = current_dir.parent / \"models\" \n",
    "encoded_dir = current_dir.parent / \"data\" / \"encoded\"\n",
    "encoded_text_file = encoded_dir / \"embedding_dict_with_price_longformer_idx.pt\"\n",
    "encoded_images_file = encoded_dir / \"images_encodings.pkl\"\n",
    "encoded_metadata_text_image_file = encoded_dir / \"item_metadata_text_image.pt\"\n",
    "auto_encoder_metadata_file = encoded_dir / \"compressed_all_data_encodings_256.pkl\"\n",
    "user_embed_model_path = models_dir / \"Yahlly_Optuna_23_2_MFBiases_0_0.9449033475350068.pth\"\n",
    "text_embeddings = torch.load(encoded_text_file)\n",
    "\n",
    "with open(encoded_images_file, 'rb') as f:\n",
    "    images_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5becac4-7be8-43c3-9cf4-d3213727da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136900f-23aa-4b6a-8480-548442e840fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### if we choose to use Auto encoder data instead of the long text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df95d1de-f2c1-4ce4-a80d-a4394052ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(auto_encoder_metadata_file, 'rb') as f:\n",
    "#     item_metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9286931c-b1be-4d6c-a37a-e823b87746c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# item_embeddings_tensor = torch.zeros(len(item_metadata), item_metadata[0].size(0))\n",
    "# for idx, (item_id, embed) in enumerate(item_metadata.items()):\n",
    "#     item_embeddings_tensor[idx] = embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "62226822-cf96-4ad8-89ee-acb3dd7c5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_encoder_metadata_file_pt_out = encoded_dir / \"compressed_all_data_encodings_256.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc08fe23-2333-40a1-a3dd-7ad519c0d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(item_embeddings_tensor,auto_encoder_metadata_file_pt_out )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b93a1-3f76-4d61-9fce-7b8781928917",
   "metadata": {},
   "source": [
    "### Load pretrained embeddings from yahlly's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d488209-682d-4f93-8a30-81097c42ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_file_path = current_dir.parent / \"data\" / \"data_and_test_files\" / \"user_item_rating_table_train_with_idx.csv\"\n",
    "df2 = pd.read_csv(user_item_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f507d9c-06f1-4e57-9dd4-83bcff78b04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MFWithBiasesFreeze(\n",
       "  (user_bias): Embedding(1096901, 1)\n",
       "  (item_bias): Embedding(198771, 1)\n",
       "  (user_embedding): Embedding(1096901, 24)\n",
       "  (item_embedding): Embedding(198771, 24)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = current_dir.parent / \"models\" / \"Yahlly_24_2_MF_Frozen_Biases_18_0.934416908145054.pth\"\n",
    "\n",
    "model = torch.load(model_path, map_location=device)  # Load the entire model object\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d35b5d-cbef-48d5-a6c5-2e7046983827",
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_user_embed = model.user_embedding\n",
    "warm_item_embed = model.item_embedding\n",
    "# initial_user_bias = model.user_bias\n",
    "# initial_item_bias = model.item_bias\n",
    "# initial_global_bias = model.global_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d87aca-6dd9-490b-9cf0-8c929d1a2d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(198771, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warm_item_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d167131-47a3-4bb5-bc75-bab0d0686a80",
   "metadata": {},
   "source": [
    "### Load item_metadata (text+image embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbe7c3c8-cf25-417f-bee2-2c5f03a87b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metadata = torch.load( encoded_dir / \"item_metadata_text_image.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1806ef-d70d-44a2-9f9c-4e0349668034",
   "metadata": {},
   "source": [
    "#### make the item metadata a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3704363-e0a7-4106-931b-93dd5d8d176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings_tensor = torch.zeros(len(item_metadata), item_metadata[0].size(0))\n",
    "for idx, (item_id, embed) in enumerate(item_metadata.items()):\n",
    "    item_embeddings_tensor[idx] = embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "903e3d7b-a612-4e95-aec4-cea73a6ff0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings_tensor=item_embeddings_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7603678d-44d9-482a-8401-20c0a03610fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the dictionary aligns with warm_item_embed indices\n",
    "images_metadata = torch.stack([images_embeddings[idx] for idx in range(len(images_embeddings))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44f306da-c2eb-4f5a-a280-66a1bafe3e6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 155636 is out of bounds for dimension 0 with size 154593",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    101\u001b[0m metadata \u001b[38;5;241m=\u001b[39m images_metadata  \u001b[38;5;66;03m# Your metadata\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_item_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 66\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(metadata, warm_item_embed, val_split, output_size, epochs, batch_size, lr, hidden_sizes)\u001b[0m\n\u001b[1;32m     63\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     64\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m meta, emb \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     67\u001b[0m     meta, emb \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mto(device), emb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     69\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[42], line 15\u001b[0m, in \u001b[0;36mItemDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[idx], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 155636 is out of bounds for dimension 0 with size 154593"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ItemDataset(Dataset):\n",
    "    def __init__(self, metadata, embeddings):\n",
    "        self.metadata = metadata\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.metadata[idx], self.embeddings[idx]\n",
    "\n",
    "class MetadataToEmbedding(nn.Module):\n",
    "    def __init__(self, input_size=3075, output_size=24, hidden_sizes=[512, 256]):\n",
    "        super(MetadataToEmbedding, self).__init__()\n",
    "        layers = []\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_model(metadata, warm_item_embed, val_split=0.2, output_size=24, epochs=10, batch_size=64, lr=1e-3, hidden_sizes=[512, 256]):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #metadata = torch.randn_like(metadata).to(device) ## COMPLETELY RANDOM METADATAAAAAAAAAAAAAAAAAAAAA\n",
    "    \n",
    "    # Map item IDs to warm embeddings\n",
    "    warm_embeddings = warm_item_embed.weight.detach().to(device)\n",
    "\n",
    "    # Train-validation split\n",
    "    val_size = int(len(metadata) * val_split)\n",
    "    train_meta, val_meta = metadata[:-val_size], metadata[-val_size:]\n",
    "    train_emb, val_emb = warm_embeddings[:-val_size], warm_embeddings[-val_size:]\n",
    "\n",
    "    train_dataset = ItemDataset(train_meta, train_emb)\n",
    "    val_dataset = ItemDataset(val_meta, val_emb)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = MetadataToEmbedding(input_size=metadata.shape[1], output_size=output_size, hidden_sizes=hidden_sizes).to(device)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    #criterion = nn.CosineEmbeddingLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for meta, emb in train_loader:\n",
    "            meta, emb = meta.to(device), emb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(meta)\n",
    "\n",
    "            #target = torch.ones(meta.size(0)).to(device)  FOR CosineEmbeddingLoss\n",
    "            #loss = criterion(outputs, emb, target)        FOR CosineEmbeddingLoss\n",
    "            loss = criterion(outputs, emb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for meta, emb in val_loader:\n",
    "                meta, emb = meta.to(device), emb.to(device)\n",
    "                outputs = model(meta)\n",
    "                # target = torch.ones(meta.size(0)).to(device) FOR CosineEmbeddingLoss\n",
    "                # val_loss += criterion(outputs, emb, target).item() FOR CosineEmbeddingLoss\n",
    "                val_loss += criterion(outputs, emb).item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss / len(train_loader):.6f}, Val Loss: {val_loss / len(val_loader):.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_metadata_to_embedding.pth\")\n",
    "\n",
    "    print(\"Training complete. Best validation loss:\", best_val_loss)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "metadata = images_metadata  # Your metadata\n",
    "model = train_model(metadata, warm_item_embed, val_split=0.2, output_size=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56775d2d-c14b-41f6-ba03-ca79da4b9095",
   "metadata": {},
   "source": [
    "### Using the images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f960dea-6b16-4d90-ac8f-9fc77ebaef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.000225, Val Loss: 0.000071\n",
      "Epoch [2/10], Train Loss: 0.000155, Val Loss: 0.000072\n",
      "Epoch [3/10], Train Loss: 0.000155, Val Loss: 0.000071\n",
      "Epoch [4/10], Train Loss: 0.000155, Val Loss: 0.000071\n",
      "Epoch [5/10], Train Loss: 0.000155, Val Loss: 0.000072\n",
      "Epoch [6/10], Train Loss: 0.000155, Val Loss: 0.000072\n",
      "Epoch [7/10], Train Loss: 0.000155, Val Loss: 0.000071\n",
      "Epoch [8/10], Train Loss: 0.000155, Val Loss: 0.000072\n",
      "Epoch [9/10], Train Loss: 0.000155, Val Loss: 0.000072\n",
      "Epoch [10/10], Train Loss: 0.000155, Val Loss: 0.000071\n",
      "Training complete. Best validation loss: 0.044301922248450865\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ItemDataset(Dataset):\n",
    "    def __init__(self, metadata, embeddings):\n",
    "        self.metadata = metadata\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.metadata[idx], self.embeddings[idx]\n",
    "\n",
    "class MetadataToEmbedding(nn.Module):\n",
    "    def __init__(self, input_size=2048, output_size=24, hidden_sizes=[512, 256]):\n",
    "        super(MetadataToEmbedding, self).__init__()\n",
    "        layers = []\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "def filter_data_with_images(metadata, warm_embeddings, images_embeddings):\n",
    "    filtered_meta, filtered_emb = [], []\n",
    "\n",
    "    for idx, meta in enumerate(metadata):\n",
    "        if idx in images_embeddings and idx < len(warm_embeddings):\n",
    "            filtered_meta.append(meta)\n",
    "            filtered_emb.append(warm_embeddings[idx])\n",
    "\n",
    "    return torch.stack(filtered_meta), torch.stack(filtered_emb)\n",
    "def train_model(metadata, warm_item_embed, val_split=0.2, output_size=24, epochs=10, batch_size=64, lr=1e-3, hidden_sizes=[512, 256]):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #metadata = torch.randn_like(metadata).to(device) ## COMPLETELY RANDOM METADATAAAAAAAAAAAAAAAAAAAAA\n",
    "    \n",
    "    # Map item IDs to warm embeddings\n",
    "    warm_embeddings = warm_item_embed.weight.detach().to(device)\n",
    "    metadata, warm_embeddings = filter_data_with_images(metadata, warm_embeddings, images_embeddings)\n",
    "    # Train-validation split\n",
    "    val_size = int(len(metadata) * val_split)\n",
    "    train_meta, val_meta = metadata[:-val_size], metadata[-val_size:]\n",
    "    train_emb, val_emb = warm_embeddings[:-val_size], warm_embeddings[-val_size:]\n",
    "\n",
    "    train_dataset = ItemDataset(train_meta, train_emb)\n",
    "    val_dataset = ItemDataset(val_meta, val_emb)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = MetadataToEmbedding(input_size=metadata.shape[1], output_size=output_size, hidden_sizes=hidden_sizes).to(device)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    #criterion = nn.CosineEmbeddingLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for meta, emb in train_loader:\n",
    "            meta, emb = meta.to(device), emb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(meta)\n",
    "\n",
    "            #target = torch.ones(meta.size(0)).to(device)  FOR CosineEmbeddingLoss\n",
    "            #loss = criterion(outputs, emb, target)        FOR CosineEmbeddingLoss\n",
    "            loss = criterion(outputs, emb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for meta, emb in val_loader:\n",
    "                meta, emb = meta.to(device), emb.to(device)\n",
    "                outputs = model(meta)\n",
    "                # target = torch.ones(meta.size(0)).to(device) FOR CosineEmbeddingLoss\n",
    "                # val_loss += criterion(outputs, emb, target).item() FOR CosineEmbeddingLoss\n",
    "                val_loss += criterion(outputs, emb).item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss / len(train_loader):.6f}, Val Loss: {val_loss / len(val_loader):.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_metadata_to_embedding.pth\")\n",
    "\n",
    "    print(\"Training complete. Best validation loss:\", best_val_loss)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "metadata = images_metadata  # Your metadata\n",
    "model = train_model(metadata, warm_item_embed, val_split=0.2, output_size=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e07c004e-7bd0-4a24-817f-cb8f4b75f7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0118, -0.0987, -0.2878,  ...,  0.4795,  0.0573,  0.1697],\n",
       "        [-0.0573, -0.1541, -0.4666,  ...,  0.8726,  0.3153,  0.4160],\n",
       "        [-0.0399, -0.1226, -0.3553,  ...,  0.0951,  0.4708,  0.4165],\n",
       "        ...,\n",
       "        [-0.0499, -0.0656, -0.4602,  ...,  0.0438,  2.5747,  0.7043],\n",
       "        [ 0.0295, -0.0730, -0.4169,  ...,  0.1011,  0.0905,  0.5914],\n",
       "        [-0.0121, -0.0562, -0.3712,  ...,  0.2762,  0.2912,  0.0817]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e1456-7c08-44c6-8e9f-96c999a7d4e2",
   "metadata": {},
   "source": [
    "### Concat image and text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a96374-caa6-44ec-9591-47e133ce9aac",
   "metadata": {},
   "source": [
    "##### Create concat of image and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2cf0177-1837-4da8-b037-230bddb2c2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Item metadata saved to /storage/yahlly/RecSys/data/encoded/item_metadata_text_image.pt with 198771 items.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # ======= Create Combined Item Metadata =======\n",
    "# item_metadata = {}\n",
    "\n",
    "# for item_id in text_embeddings.keys():\n",
    "#     text_embed = text_embeddings[item_id]  # (1027,)\n",
    "#     image_embed = images_embeddings.get(item_id, torch.zeros(2048))  # (2048,) default to zeros if missing\n",
    "\n",
    "#     # Concatenate along the feature dimension\n",
    "#     combined_embed = torch.cat([text_embed, image_embed], dim=0)  # (3075,)\n",
    "#     item_metadata[item_id] = combined_embed\n",
    "\n",
    "# # Save the combined metadata dictionary\n",
    "# metadata_save_path = encoded_dir / \"item_metadata_text_image.pt\"\n",
    "# torch.save(item_metadata, metadata_save_path)\n",
    "\n",
    "# print(f\"âœ… Item metadata saved to {metadata_save_path} with {len(item_metadata)} items.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf8da5-eb2c-4aa4-8ee3-61f05560e03e",
   "metadata": {},
   "source": [
    "### Data loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cca433a-b985-45f9-b76e-82c9b748d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Custom Dataset Class =======\n",
    "class PairwiseDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.users = dataframe[\"user_id\"].values\n",
    "        self.item1 = dataframe[\"item1_id\"].values\n",
    "        self.item2 = dataframe[\"item2_id\"].values\n",
    "        self.labels = dataframe[\"label\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.users[idx],\n",
    "            self.item1[idx],\n",
    "            self.item2[idx],\n",
    "            self.labels[idx],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df56404e-6d20-4ed8-ba55-83f84846a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Two-Tower Model (User & Item Networks) =======\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, item_metadata_dim):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        # User Tower (Embedding)\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim) \n",
    "        ### LOAD PRETRAINED USER EMBEDDINGS\n",
    "        self.user_embedding.weight.data.copy_(initial_user_embed.weight.data)\n",
    "\n",
    "        \n",
    "        # Item Tower (Using Item Metadata)\n",
    "        self.item_fc = nn.Sequential(\n",
    "            nn.Linear(item_metadata_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embedding_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, user_ids, item1_ids, item2_ids):\n",
    "        item1_ids=item1_ids.to(device)\n",
    "        item2_ids=item2_ids.to(device)\n",
    "        user_embed = self.user_embedding(user_ids)  # (batch, embedding_dim)\n",
    "        item1_embed = self.item_fc(item_embeddings_tensor[item1_ids])  # (batch, embedding_dim)\n",
    "        item2_embed = self.item_fc(item_embeddings_tensor[item2_ids])  # (batch, embedding_dim)\n",
    "        \n",
    "        return user_embed, item1_embed, item2_embed\n",
    "\n",
    "# ======= Pairwise BPR Loss =======\n",
    "\n",
    "\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, user_embed, item1_ids, item1_embed, item2_ids, item2_embed, labels):\n",
    "        \"\"\"\n",
    "        Compute Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "        - user_embed: Tensor of shape (batch_size, embed_dim), user embeddings.\n",
    "        - item1_ids: Tensor of shape (batch_size,), IDs of item1.\n",
    "        - item1_embed: Tensor of shape (batch_size, embed_dim), embeddings for item1.\n",
    "        - item2_ids: Tensor of shape (batch_size,), IDs of item2.\n",
    "        - item2_embed: Tensor of shape (batch_size, embed_dim), embeddings for item2.\n",
    "        - labels: Tensor of shape (batch_size,), IDs of the correct (positive) item.\n",
    "\n",
    "        Returns:\n",
    "        - loss: Computed BPR loss.\n",
    "        \"\"\"\n",
    "        # Convert labels to binary: 1 if item1 is the positive item, else 0\n",
    "        labels_binary = (labels == item1_ids).float()\n",
    "\n",
    "        # Compute scores\n",
    "        score1 = (user_embed * item1_embed).sum(dim=1)  # Affinity score for item1\n",
    "        score2 = (user_embed * item2_embed).sum(dim=1)  # Affinity score for item2\n",
    "\n",
    "        # Assign correct positive and negative scores based on labels_binary\n",
    "        pos_score = torch.where(labels_binary == 1, score1, score2)\n",
    "        neg_score = torch.where(labels_binary == 1, score2, score1)\n",
    "\n",
    "        # Compute BPR loss\n",
    "        loss = -torch.log(torch.sigmoid(pos_score - neg_score)).mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a6aa6ff-cc02-4bd8-8e6e-71c245517045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item1_id</th>\n",
       "      <th>item2_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50787</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>35983</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>125581</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>60296</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item1_id  item2_id  label\n",
       "0        0     50787         0      0\n",
       "1        0         1     33482      1\n",
       "2        0     35983         2      2\n",
       "3        0         3    125581      3\n",
       "4        0         4     60296      4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0102ea4-3720-46ed-88b5-ce2759ca2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(len(df) * VAL_SPLIT)\n",
    "train_df, val_df = df[:-val_size], df[-val_size:] #df[:-val_size], df[-val_size:]\n",
    "# ======= Dataloaders =======\n",
    "train_loader = DataLoader(PairwiseDataset(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(PairwiseDataset(val_df), batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80bc30b8-dbc6-4e8f-a963-a10ab0a3b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 1096901\n",
    "num_items = 198771\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f32256eb-7757-4659-8897-10d536899224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:24<00:00, 189.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6932,Val Loss = 0.6932, Val Accuracy = 0.4996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:24<00:00, 190.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.6918,Val Loss = 0.6937, Val Accuracy = 0.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:23<00:00, 192.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.6824,Val Loss = 0.6992, Val Accuracy = 0.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:24<00:00, 189.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.6592,Val Loss = 0.7113, Val Accuracy = 0.4992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:24<00:00, 189.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.6278,Val Loss = 0.7233, Val Accuracy = 0.4992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:24<00:00, 190.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.5960,Val Loss = 0.7331, Val Accuracy = 0.4989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:23<00:00, 192.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.5661,Val Loss = 0.7404, Val Accuracy = 0.4990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:23<00:00, 191.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.5382,Val Loss = 0.7466, Val Accuracy = 0.4990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:23<00:00, 192.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.5124,Val Loss = 0.7516, Val Accuracy = 0.4991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16045/16045 [01:24<00:00, 190.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.4884,Val Loss = 0.7556, Val Accuracy = 0.4989\n",
      "âœ… Model Training Complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======= Initialize Model, Loss, Optimizer =======\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TwoTowerModel(num_users, num_items, EMBEDDING_DIM, ITEM_FEATURE_DIM).to(device)\n",
    "criterion = BPRLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ======= Training Loop =======\n",
    "# ======= Training & Validation =======\n",
    "log_file = \"cold_training_log.txt\"\n",
    "\n",
    "print(\"ðŸš€ Training Model...\")\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(\"ðŸš€ Training Model...\\n\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "    \n",
    "        for user_ids, item1_ids, item2_ids,labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            user_ids, item1_ids, item2_ids,labels = user_ids.to(device), item1_ids.to(device), item2_ids.to(device), labels.to(device)\n",
    "    \n",
    "            # Forward Pass\n",
    "            user_embed, item1_embed, item2_embed = model(user_ids, item1_ids, item2_ids)\n",
    "            #print(item1_embed==item2_embed)\n",
    "            # Compute Loss\n",
    "            # print(item1_embed==item2_embed)\n",
    "            loss = criterion(user_embed,item1_ids, item1_embed,item2_ids, item2_embed, labels)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss = train_loss /len(train_loader)\n",
    "        # ======= Validation =======\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        for user_ids, item1_ids, item2_ids, labels in val_loader:\n",
    "            user_ids, item1_ids, item2_ids, labels = (\n",
    "                user_ids.to(device),\n",
    "                item1_ids.to(device),\n",
    "                item2_ids.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            user_embed, item1_embed, item2_embed = model(user_ids, item1_ids, item2_ids)\n",
    "            #print((item1_embed==item2_embed).all())\n",
    "            score1 = (user_embed * item1_embed).sum(dim=1)  # Score for item1\n",
    "            score2 = (user_embed * item2_embed).sum(dim=1)  # Score for item2\n",
    "    \n",
    "            # Determine the correct positive and negative scores based on labels\n",
    "            labels_binary = (labels == item1_ids).float()\n",
    "            #print(labels_binary)\n",
    "            pos_scores = torch.where(labels_binary == 1, score1, score2)\n",
    "            neg_scores = torch.where(labels_binary == 1, score2, score1)\n",
    "            #print(pos_scores)\n",
    "            # Check if the model correctly ranked the positive item higher\n",
    "            loss = criterion(user_embed,item1_ids, item1_embed,item2_ids, item2_embed, labels)\n",
    "            val_loss += loss.item()\n",
    "            predictions = pos_scores > neg_scores\n",
    "    \n",
    "            correct += predictions.sum().item()\n",
    "            total += predictions.shape[0]\n",
    "    \n",
    "        val_accuracy = correct / total\n",
    "        val_loss=val_loss/len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f},Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n",
    "        log.write(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ======= Save Model =======\n",
    "#torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "print(\"âœ… Model Training Complete!\")\n",
    "with open(log_file, \"a\") as log:\n",
    "    log.write(\"âœ… Model Training Complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace4db1-603b-4ce7-a916-25009f6bc3d4",
   "metadata": {},
   "source": [
    "### PREDICT ON COLD ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c269b76-b8e2-4796-826c-eb9941b72200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
