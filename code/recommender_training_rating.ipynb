{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e03fa-eb03-4d5d-b35b-eca3cf4927ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import nbimporter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optuna\n",
    "import wandb\n",
    "import importlib\n",
    "import ipynb\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a6d26-74d6-4ff6-bed4-69dabc65c14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3b241-739f-4f79-920f-9d49736ba10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f98ab3-894e-4cc1-bbe4-14ec3bc96517",
   "metadata": {},
   "source": [
    "# Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ecd09-ce03-45de-ad99-73e5ffe7a319",
   "metadata": {},
   "source": [
    "Please note - the model needs to be edited in the Model initialization part inside the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c855129-8357-4623-91a6-7fa4df419e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = \"NCF_with_Metadata_&_biases\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161c673-6291-43a6-b4ff-df9ea919171c",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e593c9-26ab-4beb-b5c6-f0d11db9696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path.cwd()\n",
    "encoded_dir = current_dir.parent / \"data\" / \"encoded\"\n",
    "user_item_dir = current_dir.parent / \"data\" / \"pre_process\"\n",
    "save_dir = current_dir.parent / \"models\" \n",
    "\n",
    "encoded_titles_file = encoded_dir / \"titles_encodings.pkl\"\n",
    "encoded_images_file = encoded_dir / \"images_encodings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895faab3-1538-4d26-91ce-caed010c8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(encoded_titles_file, 'rb') as f:\n",
    "#     titles_embeddings = pickle.load(f)\n",
    "# with open(encoded_images_file, 'rb') as f:\n",
    "#     images_embeddings = pickle.load(f)\n",
    "\n",
    "train = load_npz(user_item_dir / 'train.npz')\n",
    "val = load_npz(user_item_dir / 'val.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeeeb7f-f8f6-47be-8578-2e30c80dd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserItemDataset(Dataset):\n",
    "    def __init__(self, user_item_matrix, device):\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.indices = list(zip(*user_item_matrix.nonzero()))\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_idx, item_idx = self.indices[idx]\n",
    "        rating = self.user_item_matrix[user_idx, item_idx]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(user_idx, dtype=torch.long),\n",
    "            torch.tensor(item_idx, dtype=torch.long),\n",
    "            torch.tensor(rating, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da7fcb-18bc-4bef-850a-d8b587892108",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4f1a8-c3b7-4e4a-ae42-ad31e20bfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_loss, best_val_loss_epoch, current_val_loss, current_val_loss_epoch):\n",
    "    early_stop_flag = False \n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        best_val_loss_epoch = current_val_loss_epoch\n",
    "    else:\n",
    "        if current_val_loss_epoch - best_val_loss_epoch > patience:\n",
    "            early_stop_flag = True  # Change flag\n",
    "    return best_val_loss, best_val_loss_epoch, early_stop_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0742f5-9263-4083-ac75-a9a04176b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, criterion, optimizer, patience, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_loss_epoch = 0\n",
    "    early_stop_flag = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_samples_train = 0\n",
    "        for user_idx, item_idx, ratings in train_loader:\n",
    "            user_idx, item_idx, ratings = user_idx.to(device), item_idx.to(device), ratings.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_idx, item_idx)\n",
    "            loss = criterion(predictions, ratings)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_size = ratings.size(0)\n",
    "            train_loss += loss.item() * batch_size \n",
    "            total_samples_train += batch_size \n",
    "            \n",
    "        train_loss /= total_samples_train\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_samples_val = 0\n",
    "        with torch.no_grad():\n",
    "            for user_idx, item_idx, ratings in val_loader:\n",
    "                user_idx, item_idx, ratings = user_idx.to(device), item_idx.to(device), ratings.to(device)\n",
    "\n",
    "                predictions = model(user_idx, item_idx)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                \n",
    "                batch_size = ratings.size(0)\n",
    "                val_loss += loss.item() * batch_size \n",
    "                total_samples_val += batch_size \n",
    "\n",
    "        val_loss /= total_samples_val\n",
    "\n",
    "        if val_loss < best_val_loss:    \n",
    "            model_name = f\"{saved_model_name}_{epoch}_{val_loss}.pth\"      \n",
    "            model_path = save_dir / model_name\n",
    "            torch.save(model, model_path)\n",
    "            \n",
    "        # Early stopping check\n",
    "        best_val_loss, best_val_loss_epoch, early_stop_flag = early_stop_check(patience, best_val_loss, best_val_loss_epoch, val_loss, epoch)\n",
    "\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672b846-8e9d-41f8-9fa3-b0a036c3b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [8, 16, 24, 32, 40])\n",
    "    hidden_units = trial.suggest_categorical(\"hidden_units\", [[32, 16], [64, 32], [128, 64], [256, 128]])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    lr = trial.suggest_categorical(\"learning_rate\", [1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [1024, 2048, 4096, 8192])\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\n",
    "    epochs = 30\n",
    "\n",
    "    # W&B initialization\n",
    "    wandb.init(project=f\"RS_project_{saved_model_name}\", \n",
    "               name=f\"trial-{trial.number}\", \n",
    "               config={\"embedding_dim\": embedding_dim,\n",
    "                       \"hidden_units\": hidden_units,\n",
    "                       \"dropout\": dropout,\n",
    "                       \"batch_size\": batch_size,\n",
    "                       \"learning_rate\": lr,\n",
    "                       \"weight_decay\": weight_decay,\n",
    "                       \"alpha\": alpha})\n",
    "    \n",
    "    # Data preparation\n",
    "    train_dataset = UserItemDataset(train, device)\n",
    "    val_dataset = UserItemDataset(val, device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Load bias data\n",
    "    current_dir = Path.cwd()\n",
    "    user_item_file_path = current_dir.parent / \"data\" / \"data_and_test_files\" / \"user_item_rating_table_train_with_idx.csv\"\n",
    "    df2 = pd.read_csv(user_item_file_path)\n",
    "\n",
    "    compressed_items_encodings = encoded_dir / \"compressed_all_data_encodings_256.pkl\"\n",
    "    with open(compressed_items_encodings, 'rb') as f:\n",
    "        compressed_items_encodings = pickle.load(f)\n",
    "    \n",
    "    # Model initialization\n",
    "    model = NCFWithMetadata(num_users=train.shape[0], num_items=train.shape[1], embedding_dim=embedding_dim, hidden_units=hidden_units, dropout=dropout, alpha=alpha, df2=df2, compressed_items_encodings=compressed_items_encodings, device=device).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Train and validate\n",
    "    best_val_loss = train_model_with_hyperparams(model, train_loader, val_loader, criterion, optimizer, patience=10, epochs=epochs, device=device)\n",
    "    \n",
    "    wandb.finish()\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313cc2b-950a-4a1a-81df-1a615f3e49bd",
   "metadata": {},
   "source": [
    "# Create Optuna study using the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742aad1-46bb-49c4-9be1-b234150f0f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d2adf-ee11-4071-9daa-5870784a563c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b3aa0-0712-41b0-96a7-a34212ed299f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf851c-d37d-4f1c-9d07-ea5495eefef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
